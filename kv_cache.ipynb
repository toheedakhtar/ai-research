{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1e645c",
   "metadata": {},
   "source": [
    "# MHA with KV cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5013060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb08395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80375c56",
   "metadata": {},
   "source": [
    "## Multi head attention without KV Caching and Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fe413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads \n",
    "        self.d_kv = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch, seq, dim = x.shape\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # each head token is attended to\n",
    "        Q = Q.view(batch, seq, self.num_heads, self.d_kv).transpose(1,2) \n",
    "        K = K.view(batch, seq, self.num_heads, self.d_kv).transpose(1,2) \n",
    "        V = V.view(batch, seq, self.num_heads, self.d_kv).transpose(1,2) \n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_kv)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_outputs = torch.matmul(attn_weights , V)\n",
    "\n",
    "        attn_outputs = attn_outputs.transpose(1, 2).contiguous()\n",
    "        attn_outputs = attn_outputs.view(batch, seq, self.d_model)\n",
    "\n",
    "        output = self.W_o(attn_outputs)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530327d",
   "metadata": {},
   "source": [
    "## MHA with KV caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2671ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_KVCache(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads \n",
    "        self.d_kv = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None, use_cache=None):\n",
    "        \"\"\"\n",
    "        x : batch, seq, d_model\n",
    "        kv_cache : dict with K, V keys \n",
    "        \"\"\"\n",
    "\n",
    "        batch, seq, dim = x.shape\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # each head token is attended to\n",
    "        Q = Q.view(batch, seq, self.num_heads, self.d_kv).transpose(1,2) \n",
    "        K = K.view(batch, seq, self.num_heads, self.d_kv).transpose(1,2) \n",
    "        V = V.view(batch, seq, self.num_heads, self.d_kv).transpose(1,2) \n",
    "\n",
    "        if kv_cache is not None:\n",
    "            print( f\"Using KV cache: \"\n",
    "                f\"K cache shape {kv_cache['k'].shape}, \"\n",
    "                f\"V cache shape {kv_cache['v'].shape}\")\n",
    "            \n",
    "            # appends newly computed keys and values with previous cached ones\n",
    "            K = torch.concat([kv_cache[\"k\"], K], dim=2)\n",
    "            V = torch.concat([kv_cache[\"v\"], V], dim=2)\n",
    "        \n",
    "        if use_cache: \n",
    "            new_kv_cache = {\n",
    "                # removes K and V from computation graph\n",
    "                \"k\" : K.detach(),\n",
    "                \"v\" : V.detach()\n",
    "            }\n",
    "        else: \n",
    "            new_kv_cache = None\n",
    "        \n",
    "        print(f\"Q shape: {Q.shape}\")\n",
    "        print(f\"K shape (after cache): {K.shape}\")\n",
    "        print(f\"V shape (after cache): {V.shape}\")\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_kv)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_outputs = torch.matmul(attn_weights , V)\n",
    "\n",
    "        attn_outputs = attn_outputs.transpose(1, 2).contiguous()\n",
    "        attn_outputs = attn_outputs.view(batch, seq, self.d_model)\n",
    "\n",
    "        output = self.W_o(attn_outputs)\n",
    "        return output, new_kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09489159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- step 0 -------------\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 1, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 1, 8])\n",
      "\n",
      "--------- step 1 -------------\n",
      "Using KV cache: K cache shape torch.Size([2, 8, 1, 8]), V cache shape torch.Size([2, 8, 1, 8])\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 2, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 2, 8])\n",
      "\n",
      "--------- step 2 -------------\n",
      "Using KV cache: K cache shape torch.Size([2, 8, 2, 8]), V cache shape torch.Size([2, 8, 2, 8])\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 3, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 3, 8])\n",
      "\n",
      "--------- step 3 -------------\n",
      "Using KV cache: K cache shape torch.Size([2, 8, 3, 8]), V cache shape torch.Size([2, 8, 3, 8])\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 4, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 4, 8])\n",
      "\n",
      "--------- step 4 -------------\n",
      "Using KV cache: K cache shape torch.Size([2, 8, 4, 8]), V cache shape torch.Size([2, 8, 4, 8])\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 5, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "batch = 2 \n",
    "num_heads = 8 \n",
    "seq = 10 \n",
    "d_model = 64 \n",
    "\n",
    "mha_kv = MultiHeadAttention_KVCache(d_model, num_heads)\n",
    "\n",
    "kv_cache = None \n",
    "\n",
    "x = torch.randn(batch, seq, d_model)\n",
    "\n",
    "# training\n",
    "# for i in range(5):\n",
    "#     print(f\"\\n--------- step {i} -------------\")\n",
    "#     output , kv_cache = mha_kv(x, kv_cache=None, use_cache=False)\n",
    "\n",
    "# inference\n",
    "for i in range(5):\n",
    "    print(f\"\\n--------- step {i} -------------\")\n",
    "    x_token = x[:, i:i+1, :]   # shape: (batch, 1, d_model)\n",
    "    output , kv_cache = mha_kv(x_token, kv_cache=kv_cache, use_cache=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b7f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 4\n",
      "k numel : 640\n",
      "v numel : 640\n",
      "raw byte size : 5120\n",
      "size in mb : 0.0048828125\n"
     ]
    }
   ],
   "source": [
    "def calculate_kv_cache_size(kv_cache):\n",
    "    k = kv_cache[\"k\"]\n",
    "    v = kv_cache[\"v\"]\n",
    "\n",
    "    precision = k.element_size()\n",
    "    print(f\"precision : {precision}\")\n",
    "    \n",
    "    total_bytes = (k.numel() + v.numel()) * precision\n",
    "    print(f\"k numel : {k.numel()}\\nv numel : {v.numel()}\")\n",
    "    return total_bytes\n",
    "\n",
    "size_bytes = calculate_kv_cache_size(kv_cache)\n",
    "print(f\"raw byte size : {size_bytes}\")\n",
    "size_mb = size_bytes / (1024**2)\n",
    "print(f\"size in mb : {size_mb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662c772",
   "metadata": {},
   "source": [
    "## with causal masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c037611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_KVCache_Masked(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads \n",
    "        self.d_kv = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None, use_cache=None):\n",
    "        \"\"\"\n",
    "        x : batch, seq, d_model\n",
    "        kv_cache : dict with K, V keys \n",
    "        use_cache : return kv cache \n",
    "        \"\"\"\n",
    "\n",
    "        batch, seq, dim = x.shape\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # each head token is attended to\n",
    "        Q = Q.view(batch, seq, self.num_heads, self.d_kv).transpose(1,2) \n",
    "        K = K.view(batch, seq, self.num_heads, self.d_kv).transpose(1,2) \n",
    "        V = V.view(batch, seq, self.num_heads, self.d_kv).transpose(1,2) \n",
    "\n",
    "        past_len = 0\n",
    "        if kv_cache is not None:\n",
    "            print( f\"Using KV cache: \"\n",
    "                f\"K cache shape {kv_cache['k'].shape}, \"\n",
    "                f\"V cache shape {kv_cache['v'].shape}\")\n",
    "            past_len = kv_cache[\"k\"].shape[2]\n",
    "            # appends newly computed keys and values with previous cached ones\n",
    "            K = torch.concat([kv_cache[\"k\"], K], dim=2)\n",
    "            V = torch.concat([kv_cache[\"v\"], V], dim=2)\n",
    "        \n",
    "        if use_cache: \n",
    "            new_kv_cache = {\n",
    "                # removes K and V from computation graph\n",
    "                \"k\" : K.detach(),\n",
    "                \"v\" : V.detach()\n",
    "            }\n",
    "        else: \n",
    "            new_kv_cache = None\n",
    "        \n",
    "        # causal masking \n",
    "        # q len = seq, K length = past + seq \n",
    "        # Keys (K) include both previously cached tokens (past_len) and the newly processed tokens (seq).\n",
    "        total_len = past_len + seq \n",
    "\n",
    "        # causal_mask = torch.triu(\n",
    "        #         torch.ones(seq, total_len, device=x.device, dtype=torch.bool),\n",
    "        #         diagonal=past_len + 1\n",
    "        #     )\n",
    "        # # -inf where True\n",
    "        # attn_mask = torch.zeros(seq, total_len, device=x.device)\n",
    "        # attn_mask.masked_fill_(causal_mask, float('-inf'))\n",
    "        # # for brodcasting -> batch , heads, seq, total_len\n",
    "        # attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        print(f\"Q shape: {Q.shape}\")\n",
    "        print(f\"K shape (after cache): {K.shape}\")\n",
    "        print(f\"V shape (after cache): {V.shape}\")\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_kv)\n",
    "        # all masking here\n",
    "        if seq > 1:\n",
    "            scores = scores.masked_fill(torch.triu(torch.ones(seq, total_len, dtype=torch.bool), diagonal=past_len+1), float(\"-inf\"))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_outputs = torch.matmul(attn_weights , V)\n",
    "\n",
    "        attn_outputs = attn_outputs.transpose(1, 2).contiguous()\n",
    "        attn_outputs = attn_outputs.view(batch, seq, self.d_model)\n",
    "\n",
    "        output = self.W_o(attn_outputs)\n",
    "        return output, new_kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "939df81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- step 0 -------------\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 1, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 1, 8])\n",
      "\n",
      "--------- step 1 -------------\n",
      "Using KV cache: K cache shape torch.Size([2, 8, 1, 8]), V cache shape torch.Size([2, 8, 1, 8])\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 2, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 2, 8])\n",
      "\n",
      "--------- step 2 -------------\n",
      "Using KV cache: K cache shape torch.Size([2, 8, 2, 8]), V cache shape torch.Size([2, 8, 2, 8])\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 3, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 3, 8])\n",
      "\n",
      "--------- step 3 -------------\n",
      "Using KV cache: K cache shape torch.Size([2, 8, 3, 8]), V cache shape torch.Size([2, 8, 3, 8])\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 4, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 4, 8])\n",
      "\n",
      "--------- step 4 -------------\n",
      "Using KV cache: K cache shape torch.Size([2, 8, 4, 8]), V cache shape torch.Size([2, 8, 4, 8])\n",
      "Q shape: torch.Size([2, 8, 1, 8])\n",
      "K shape (after cache): torch.Size([2, 8, 5, 8])\n",
      "V shape (after cache): torch.Size([2, 8, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "batch = 2 \n",
    "num_heads = 8 \n",
    "seq = 10 \n",
    "d_model = 64 \n",
    "\n",
    "mha_kv = MultiHeadAttention_KVCache_Masked(d_model, num_heads)\n",
    "\n",
    "kv_cache = None \n",
    "\n",
    "x = torch.randn(batch, seq, d_model)\n",
    "\n",
    "# training\n",
    "# for i in range(5):\n",
    "#     print(f\"\\n--------- step {i} -------------\")\n",
    "#     output , kv_cache = mha_kv(x, kv_cache=None, use_cache=False)\n",
    "\n",
    "# inference\n",
    "for i in range(5):\n",
    "    print(f\"\\n--------- step {i} -------------\")\n",
    "    x_token = x[:, i:i+1, :]   # shape: (batch, 1, d_model)\n",
    "    output , kv_cache = mha_kv(x_token, kv_cache=kv_cache, use_cache=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9377d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 4\n",
      "k numel : 640\n",
      "v numel : 640\n",
      "raw byte size : 5120\n",
      "size in mb : 0.0048828125\n"
     ]
    }
   ],
   "source": [
    "def calculate_kv_cache_size(kv_cache):\n",
    "    k = kv_cache[\"k\"]\n",
    "    v = kv_cache[\"v\"]\n",
    "\n",
    "    precision = k.element_size()\n",
    "    print(f\"precision : {precision}\")\n",
    "    \n",
    "    total_bytes = (k.numel() + v.numel()) * precision\n",
    "    print(f\"k numel : {k.numel()}\\nv numel : {v.numel()}\")\n",
    "    return total_bytes\n",
    "\n",
    "size_bytes = calculate_kv_cache_size(kv_cache)\n",
    "print(f\"raw byte size : {size_bytes}\")\n",
    "size_mb = size_bytes / (1024**2)\n",
    "print(f\"size in mb : {size_mb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b467c1",
   "metadata": {},
   "source": [
    "### what masking does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a9af8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n",
      "tensor([[ 0.3363,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.0253,  0.9355,    -inf,    -inf,    -inf],\n",
      "        [ 0.8504, -1.1353,  0.6112,    -inf,    -inf],\n",
      "        [ 1.3716, -1.2834, -0.2104,  0.7902,    -inf],\n",
      "        [ 1.2539,  0.3034, -0.9586,  1.9031,  0.3068]])\n"
     ]
    }
   ],
   "source": [
    "randomM = torch.randn(5,5)\n",
    "mask = torch.triu(torch.ones(5, 5, dtype=torch.bool), diagonal=1)\n",
    "print(mask)\n",
    "masked_rM = randomM.masked_fill(mask, float(\"-inf\"))\n",
    "print(masked_rM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
