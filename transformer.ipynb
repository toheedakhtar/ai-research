{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1458735",
   "metadata": {},
   "source": [
    "# Decoder only Transformer / GPT \n",
    "\n",
    "**Components** \n",
    "1. Masked MHA (Multi head Attention) : Attention Mechanism \n",
    "2. Feed Forward Netwwork : Two nn.Linear with activation\n",
    "3. Layer Norm : handles vanishing gradients / 0 mean 1 variance / Normalizes activation\n",
    "4. Residual connections : Adds inputs to outputs (x + f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c9f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd22b6",
   "metadata": {},
   "source": [
    "## Multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d14ef2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    \"\"\"Multi head attention (masked)\"\"\" \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model \n",
    "        self.num_heads = num_heads \n",
    "        self.d_k = d_model // num_heads \n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        batch, seq, d_model = x.shape\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(batch, seq, self.num_heads, self.d_k).transpose(1,2) # seq for each head\n",
    "        K = K.view(batch, seq, self.num_heads, self.d_k).transpose(1,2) \n",
    "        V = V.view(batch, seq, self.num_heads, self.d_k).transpose(1,2) \n",
    "\n",
    "        scores = torch.matmul(Q , K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None: \n",
    "            # mask == float(\"-inf\") creates a boolean tensor of shape mask, where mask has value -inf, it is set to True\n",
    "            # then wherever condition is set to true, -inf it\n",
    "            # masked fill needs boolean value. \n",
    "            scores = scores.masked_fill(mask == float(\"-inf\"), float(\"-inf\"))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        # print(f\"attn weights sum : {attn_weights.sum(dim=-1)}\")\n",
    "        attn_outputs = torch.matmul(attn_weights, V)\n",
    "\n",
    "        attn_output = attn_outputs.transpose(1, 2).contiguous() # merging heads\n",
    "        attn_output = attn_output.view(batch, seq, d_model)\n",
    "\n",
    "        out = self.W_o(attn_output)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d449997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape : torch.Size([4, 10, 64])\n",
      "out.shape : torch.Size([4, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "d_model = 64\n",
    "heads = 8\n",
    "seq = 10\n",
    "batch = 4\n",
    "\n",
    "\n",
    "causal_mask = torch.triu(torch.ones(seq, seq), diagonal=1)\n",
    "# print(causal_mask)\n",
    "causal_mask = causal_mask.masked_fill(causal_mask == 1, float(\"-inf\"))\n",
    "# print(causal_mask)\n",
    "\n",
    "mha = MHA(d_model, heads)\n",
    "x = torch.randn(batch, seq, d_model)\n",
    "out = mha(x)\n",
    "\n",
    "print(f\"x.shape : {x.shape}\")\n",
    "print(f\"out.shape : {out.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120cadb",
   "metadata": {},
   "source": [
    "## FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a77add18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"Feed Forward Network\"\"\"\n",
    "    # enhances model's capability to learn by expanding and then compressing dims\n",
    "    # kind of enriches attention output, makes them more expressive, richer representation of tokens\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83833023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape : torch.Size([2, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "seq = 10\n",
    "d_ff = 256 \n",
    "d_model = 64\n",
    "\n",
    "ff = FFN(d_model, d_ff)\n",
    "x = torch.randn(batch, seq, d_model)\n",
    "out = ff(x)\n",
    "\n",
    "print(f\"out.shape : {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bdf02",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51f26e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"decoder block, combines mha, ffn, layer norm, residual connections\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MHA(d_model, num_heads)\n",
    "        self.feedforward = FFN(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # attn with residual and norm\n",
    "        attn_out = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        # ffn with residual and norm \n",
    "        ff_o = self.feedforward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_o))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b915b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape : torch.Size([2, 10, 64])\n",
      "out shape : torch.Size([2, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "d_model = 64 \n",
    "d_ff = 256\n",
    "seq - 10 \n",
    "batch = 2 \n",
    "heads = 8 \n",
    "\n",
    "causal_mask = torch.triu(torch.ones(seq, seq), diagonal=1) # True at 1s\n",
    "causal_mask = causal_mask.masked_fill(causal_mask == 1, float(\"-inf\")) # -inf at True\n",
    "\n",
    "decoder_block = DecoderBlock(d_model, heads, d_ff)\n",
    "x = torch.randn(batch, seq, d_model)\n",
    "out = decoder_block(x, causal_mask)\n",
    "\n",
    "print(f\"x shape : {x.shape}\")\n",
    "print(f\"out shape : {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da21261",
   "metadata": {},
   "source": [
    "# Decoder Only Transformer - implemented\n",
    "Stacking multiple decoder blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eed5c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer(nn.Module):\n",
    "    \"\"\"Decoder only Transformer / GPT \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, seq, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(seq, d_model) # seq is max_seq_len\n",
    "        # stacking blocks\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False) # output projection [batch, seq, vocab_size]\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x: Token [batch, seq]\"\"\"\n",
    "\n",
    "        batch, seq = x.shape\n",
    "\n",
    "        mask = torch.triu(torch.ones(seq, seq), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "\n",
    "        token_emb = self.token_emb(x) # batch, seq, d_model / maps token id to vector of size d_model\n",
    "        positions = torch.arange(seq).unsqueeze(0) # adds batch dim to positions\n",
    "        pos_emb = self.pos_emb(positions) # 1, seq, d_model / maps positoin to vector of size d_model\n",
    "\n",
    "        x = self.dropout(token_emb + pos_emb) # brodcast pos_emb over batch [batch, seq, d_model] / dropout for regularization\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x) # gives raw unnormalized scores which are converted to probabs called logits \n",
    "\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6ec241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 20])\n",
      "logits shape : torch.Size([2, 20, 500])\n",
      "total params: 271232\n"
     ]
    }
   ],
   "source": [
    "vocab = 500\n",
    "d_model = 64 \n",
    "d_ff = 256\n",
    "heads = 8 \n",
    "num_layers = 4 \n",
    "seq_len = 128 # max_seq_len \n",
    "\n",
    "model = DecoderTransformer(vocab, d_model, heads, num_layers, d_ff, seq_len, dropout=0.1)\n",
    "\n",
    "batch = 2 \n",
    "seq = 20 # indices\n",
    "x = torch.randint(0, vocab, (batch, seq))\n",
    "print(f\"x: {x.shape}\")\n",
    "# print(x)\n",
    "\n",
    "logits = model(x)\n",
    "print(f\"logits shape : {logits.shape}\")\n",
    "print(f\"total params: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe6014f",
   "metadata": {},
   "source": [
    "implemented decoder only transformer / GPT style"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
