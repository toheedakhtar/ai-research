{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a573d7",
   "metadata": {},
   "source": [
    "# Multi Head Attention or MHA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d43e0",
   "metadata": {},
   "source": [
    "as the name, multiple attention mechanisms, each attending to different things / information in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740e8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64 \n",
    "num_heads = 8 \n",
    "d_k = d_model // num_heads # dimension of per head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc36a3",
   "metadata": {},
   "source": [
    "MHA : \n",
    "1. splits Q, K, V in multiple heads \n",
    "2. Apply attention to each head independently\n",
    "3. Concatenate all heads\n",
    "4. apply output proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894205eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47db1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0 \n",
    "\n",
    "        self.d_model = d_model \n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # output projection \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : [batch-size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # shape after -> [batch_sizem, num_heads, seq_len, d_k] \n",
    "        # splits d_model into heads * d_k, this gives multiple parallel views of the sequence\n",
    "        # each head with its own attn scores\n",
    "        # if not seperrated the denominator in softmax (attn fx) will be d_model instead of d_k, \n",
    "        # which will result in one giant head\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        # [batch_size, num_head, seq_len, d_k] @ [batch_size, num_head, d_k, seq_len] -> [batch_size, num_head, seq_len, seq_len]\n",
    "        # scaled dot product attention \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k) \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attn_outputs = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # concatenate heads : [batch_size, seq_len, d_model]\n",
    "        attn_outputs = attn_outputs.transpose(1, 2).contiguous()\n",
    "        attn_outputs = attn_outputs.view(batch_size, seq_len, d_model)\n",
    "\n",
    "        # passing output through a linear layer \n",
    "        output = self.W_o(attn_outputs) \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aaf8d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([2, 10, 64])\n",
      "Output shape : torch.Size([2, 10, 64])\n",
      "no of heads : 8\n"
     ]
    }
   ],
   "source": [
    "d_model = 64 \n",
    "num_heads = 8 \n",
    "seq_len = 10 \n",
    "batch_size = 2 \n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output = mha(x)\n",
    "\n",
    "print(f'Input shape : {x.shape}')\n",
    "print(f'Output shape : {output.shape}')\n",
    "print(f'no of heads : {num_heads}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
